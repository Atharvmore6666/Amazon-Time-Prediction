# -*- coding: utf-8 -*-
"""Amazon Time Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/158F-JvT_q9zBN_343Ce3HpU1DBUfi530

# **Project Name**    - Amazon Time Prediction

##### **Project Type**    - EDA/Regression
##### **Contribution**    - Individual
##### **Name -**           Atharv More

# **Project Summary -**

Delivery Time Prediction
This project aims to develop a predictive model capable of estimating the total delivery time (in minutes) for a logistics service. The analysis is based on a dataset containing 43,739 records and 16 features.

Initial data exploration highlighted the need for significant feature engineering. Specifically, geographical features (Store_Latitude and Drop_Latitude) must be converted into a single, meaningful feature, Travel_Distance_km, using the Haversine formula. Similarly, raw time features must be converted into datetime objects to calculate crucial metrics like Time_to_Pickup_minutes and to extract cyclical components (like sine/cosine of the order hour).

The data quality is high, with only minor missing values (under 0.25%) in Agent_Rating and Weather, which were designated for imputation via median and mode, respectively. The overall data preparation plan includes converting data types, engineering distance and temporal features, encoding categorical variables (Ordinal for Traffic, One-Hot for others), and scaling all final numerical inputs to ensure the model receives standardized data. This structured approach is designed to maximize the predictive performance of the final machine learning model.

# **GitHub Link -**

https://github.com/Atharvmore6666/Amazon-Time-Prediction

# **Problem Statement**

The primary objective is to accurately predict the Estimated Time of Delivery (ETD) for Amazon orders using a comprehensive dataset that includes location data, agent details, time of order, and real-time environmental factors (traffic and weather). The prediction must be a continuous variable (Delivery Time in hours), framed as a regression problem.

The solution must quantify the impact of different features—especially distance, agent performance, and environmental conditions—on the final delivery time to provide actionable insights for logistics optimization, improve service reliability, and enhance customer satisfaction by setting precise delivery expectations.

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries
"""

# Import Libraries
# Standard data manipulation and analysis libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, time
import warnings
warnings.filterwarnings('ignore')
import math

# Statistical testing
from scipy import stats

# Modeling libraries
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import xgboost as xgb
# Set up a random seed for reproducibility
RANDOM_SEED = 42

print("Required libraries imported.")

"""### Dataset Loading"""

# Load Dataset
# Load Dataset
DATA_PATH = "/content/drive/MyDrive/Labmentix/Amazon Delivery Prediction/amazon_delivery.csv"
try:
    df = pd.read_csv(DATA_PATH)
    print(f"Dataset successfully loaded from {DATA_PATH}")
except FileNotFoundError:
    print(f"Error: {DATA_PATH} not found. Please ensure the file is uploaded.")
    df = None # Assign None if loading fails

"""### Dataset First View"""

# Dataset First Look
df.head(10)

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")

"""### Dataset Information"""

# Dataset Info
df.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
print(f"Number of duplicate rows: {df.duplicated().sum()}")

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
df.isnull().sum()

# Visualizing the missing values
# Create a heatmap to visualize missing values
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Values Heatmap')
plt.show()

"""### What did you know about your dataset?

1. Initial Data Exploration (from df.info())
   The dataset contains 43,739 entries across 16 columns. The target variable for prediction is Delivery_Time (int64).
Key Findings and Initial Observations:
A. Data Types & Feature Use
Feature Type

Columns

Observation

Target

Delivery_Time (int64)

Integer, representing time in minutes (assumed).

Numerical

Agent_Age (int64), Agent_Rating (float64)

Agent-specific features. Agent_Rating has missing values.

Location

Store_Latitude, Store_Longitude, Drop_Latitude, Drop_Longitude (all float64)

These coordinates need to be transformed using the Haversine formula to create a single Travel_Distance feature.

Date/Time

Order_Date, Order_Time, Pickup_Time (all object)

Currently strings/objects; must be converted to datetime objects for calculating time differences (like waiting time) and extracting cyclical features (hour, day of week).

Categorical

Order_ID, Weather, Traffic, Vehicle, Area, Category (all object)

These will require appropriate encoding (Ordinal or One-Hot) before modeling. Order_ID is an identifier and should be dropped.

B. Missing Values
There are two columns with missing data, both representing a very small percentage of the total dataset:

Agent_Rating: 54 missing values out of 43,739 (approx. 0.12%).

Plan: Impute with the median or mean.

Weather: 91 missing values (approx. 0.21%).

Plan: Impute with the mode or assign an 'Unknown' category.

2. Data Preparation Steps (Planned)
  Based on the initial analysis, the following steps, detailed in the data_preparation_plan.md file, are required:

  Type Conversion: Convert all date/time columns to datetime objects.

  Feature Engineering:

Calculate Travel_Distance_km from latitude/longitude pairs (Haversine distance).

Calculate Time_to_Pickup_minutes by subtracting Order_Time from Pickup_Time.

Extract temporal features (Order_Hour, Order_DayOfWeek, etc.).

Encoding: Apply ordinal encoding to Traffic and One-Hot Encoding to other categorical features.

Scaling: Scale all resulting numerical features.

## ***2. Understanding Your Variables***
"""

# Dataset Columns
df.columns

# Dataset Describe

"""### Variables Description

| Variable        | Min / Max   | Mean / Median | Description                                                                                                                               |
|-----------------|-------------|---------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| Agent_Age       | 15 / 50     | Mean ≈29.6, Median =30 | The age of the delivery agent, ranging from 15 to 50 years. The distribution is close to symmetric around 30.                         |
| Agent_Rating    | 1.0 / 6.0   | Mean ≈4.63, Median =4.7 | The agent's performance rating, primarily concentrated in the high 4.0s (mean and median). The minimum of 1.0 suggests some very low-performing agents. |
| Store_Latitude  | -30.9 / 30.9 | Mean ≈17.2, Median ≈18.5 | The starting point latitude. The wide range and the fact that the minimum is negative suggest a global or hemisphere-spanning dataset. |
| Store_Longitude | -88.4 / 88.4 | Mean ≈70.7, Median ≈75.9 | The starting point longitude. Again, this indicates a potentially broad geographic scope.                                             |
| Drop_Latitude   | 0.01 / 31.05 | Mean ≈17.46, Median ≈18.63 | The destination latitude. Similar distribution to Store_Latitude.                                                                         |
| Drop_Longitude  | 0.01 / 88.56 | Mean ≈70.82, Median ≈76.0 | The destination longitude. Similar distribution to Store_Longitude.                                                                       |
| Delivery_Time   | 10 / 270    | Mean ≈124.9, Median =125 | The time taken for delivery in minutes (assumed). The delivery times are highly centered around 125 minutes (just over 2 hours), but range from 10 minutes to 4.5 hours (270 minutes). |

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.
for column in df.columns:
    unique_values = df[column].unique()
    print(f"Unique values for {column}: {unique_values}")

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

df.isnull().sum()

#Impute Missing Values
median_rating = df['Agent_Rating'].median()
df['Agent_Rating'].fillna(median_rating, inplace=True)

# Impute Weather (Categorical) with Mode
mode_weather = df['Weather'].mode()[0]
df['Weather'].fillna(mode_weather, inplace=True)
print("Missing values imputed for Agent_Rating and Weather.")

# 3. Convert Date/Time Columns
    # Create combined datetime objects for calculation
    # NOTE: Assuming Order_Time and Pickup_Time are relative to Order_Date
df['Order_Datetime'] = pd.to_datetime(df['Order_Date'] + ' ' + df['Order_Time'], errors='coerce')
df['Pickup_Datetime'] = pd.to_datetime(df['Order_Date'] + ' ' + df['Pickup_Time'], errors='coerce')
df.drop(columns=['Order_Date', 'Order_Time', 'Pickup_Time'], inplace=True)
print("Date/Time columns converted and combined.")

df

#  Feature Engineering: Haversine Distance (Travel Distance)
from math import radians, sin, cos, sqrt, atan2

def haversine(lat1, lon1, lat2, lon2):
        """Calculate the great-circle distance between two points on the Earth."""
        R = 6371  # Radius of Earth in kilometers
        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])

        dlon = lon2 - lon1
        dlat = lat2 - lat1

        a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
        c = 2 * atan2(sqrt(a), sqrt(1 - a))
        distance = R * c
        return distance

df['Travel_Distance_km'] = df.apply(
        lambda row: haversine(
            row['Store_Latitude'], row['Store_Longitude'],
            row['Drop_Latitude'], row['Drop_Longitude']
        ), axis=1
    )
print("Haversine distance calculated (Travel_Distance_km).")

#  Feature Engineering: Time to Pickup (Operational Efficiency)
    # Calculate difference in seconds, then convert to minutes
time_diff = df['Pickup_Datetime'] - df['Order_Datetime']
# Check for cases where Pickup is recorded *before* Order (data quality issue)
invalid_pickup_time = (time_diff < pd.Timedelta(seconds=0)).sum()
if invalid_pickup_time > 0:
    print(f"WARNING: {invalid_pickup_time} records show pickup before order. Setting these Time_to_Pickup to 0.")
    time_diff = time_diff.clip(lower=pd.Timedelta(seconds=0))
df['Time_to_Pickup_minutes'] = time_diff.dt.total_seconds() / 60
print("Time_to_Pickup_minutes calculated.")

# Feature Engineering: Temporal Features (Cyclical and Day-based)
df['Order_Hour'] = df['Order_Datetime'].dt.hour
df['Order_DayOfWeek'] = df['Order_Datetime'].dt.dayofweek # Monday=0, Sunday=6
df['Is_Weekend'] = df['Order_DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)

# Cyclical encoding for Order_Hour to capture 23:00 being close to 00:00
df['Order_Hour_sin'] = np.sin(2 * np.pi * df['Order_Hour'] / 24)
df['Order_Hour_cos'] = np.cos(2 * np.pi * df['Order_Hour'] / 24)
print("Temporal features extracted and cyclically encoded.")

df

#  Drop high-granularity and engineered columns
cols_to_drop = [
    'Order_ID', 'Store_Latitude', 'Store_Longitude',
    'Drop_Latitude', 'Drop_Longitude',
    'Order_Datetime', 'Pickup_Datetime',
    'Order_Hour' # Dropping raw hour after cyclical encoding
]
df.drop(columns=cols_to_drop, inplace=True, errors='ignore')
print(f"Dropped high-granularity/raw location columns: {cols_to_drop}.")

"""Answer Here.

**Manipulations Performed:**
1.  **Duplicate Handling:** Checked and dropped any duplicate rows.
2.  **Missing Value Imputation:** `Agent_Rating` was filled with the median, and `Weather` was filled with the mode.
3.  **Data Type Conversion:** Converted `Order_Date`, `Order_Time`, and `Pickup_Time` strings into usable datetime objects for calculation.
4.  **Feature Engineering (Location):** Calculated `Travel_Distance_km` using the Haversine formula from the four coordinate columns. This transforms four non-linear features into one powerful linear feature.
5.  **Feature Engineering (Operational):** Calculated `Time_to_Pickup_minutes` (waiting time), which is a crucial metric for operational efficiency and likely a strong predictor of total delivery time. Handled illogical negative time differences.
6.  **Feature Engineering (Temporal):** Extracted `Order_DayOfWeek` and created cyclical sine/cosine features for `Order_Hour` to better represent the continuous nature of time.
7.  **Column Dropping:** Removed unique identifier (`Order_ID`), raw coordinate columns, and the original time/date columns after engineering.

**Insights Found:**
* Operational issues exist: A small number of records showed `Pickup_Time` earlier than `Order_Time`, indicating data quality issues, which were safely handled.
* The newly created `Time_to_Pickup_minutes` is a direct measure of internal process efficiency that will be highly correlated with `Delivery_Time`.
* The Haversine distance feature is ready to be analyzed for its relationship with delivery time.

## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

#### Chart - 1
"""

plt.figure(figsize=(10, 6))
sns.histplot(df['Delivery_Time'], kde=True, bins=30, color='skyblue')
plt.title('Chart 1: Distribution of Delivery Time (Target Variable)')
plt.xlabel('Delivery Time (minutes)')
plt.ylabel('Count')
plt.show()

"""#### Chart - 2"""

# Chart - 2 visualization code
plt.figure(figsize=(8, 5))
sns.histplot(df['Agent_Age'], bins=25, kde=False, color='darkorange')
plt.title('Chart 2: Distribution of Agent Age')
plt.xlabel('Agent Age')
plt.ylabel('Count')
plt.show()

"""#### Chart - 3"""

# Chart - 3 visualization code
plt.figure(figsize=(8, 5))
sns.histplot(df['Agent_Rating'], bins=20, kde=True, color='purple')
plt.title('Chart 3: Distribution of Agent Ratings')
plt.xlabel('Agent Rating (1.0 to 6.0)')
plt.ylabel('Count')
plt.show()

df.Agent_Rating.unique()

"""#### Chart - 4"""

# Chart - 4 visualization code
plt.figure(figsize=(10, 6))
sns.histplot(df['Travel_Distance_km'], bins=50, kde=True, color='teal')
plt.title('Chart 4: Distribution of Haversine Travel Distance')
plt.xlabel('Distance (km)')
plt.ylabel('Count')
plt.xlim(0, df['Travel_Distance_km'].quantile(0.99)) # Limiting for better visual
plt.show()

"""#### Chart - 5"""

# Check the unique values and their counts
print(df['Traffic'].value_counts())

# Check just the unique values
print(df['Traffic'].unique())

# 1. Strip trailing spaces from the 'Traffic' column
df['Traffic'] = df['Traffic'].str.strip()

# 2. Convert the 'NaN' string values to the actual NaN
# This allows you to exclude them from the plot or handle them properly.
import numpy as np
df['Traffic'] = df['Traffic'].replace('NaN', np.nan)

# 3. Drop NaN values for plotting (if you want to exclude them)
df_clean = df.dropna(subset=['Traffic'])

# Now, use the corrected and cleaned order for plotting
plt.figure(figsize=(10, 6))
sns.boxplot(x='Traffic', y='Delivery_Time', data=df_clean, order=['Low', 'Medium', 'High', 'Jam'])
plt.title('Chart 5: Delivery Time vs. Traffic Condition')
plt.xlabel('Traffic Condition')
plt.ylabel('Delivery Time (minutes)')

"""#### Chart - 6"""

# Chart - 6 visualization code
plt.figure(figsize=(12, 6))
sns.violinplot(x='Weather', y='Delivery_Time', data=df, inner='quartile', palette='Set2')
plt.title('Chart 6: Delivery Time vs. Weather Condition')
plt.xlabel('Weather Condition')
plt.ylabel('Delivery Time (minutes)')
plt.xticks(rotation=45)
plt.show()

"""#### Chart - 7"""

# Chart - 7 visualization code
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Time_to_Pickup_minutes', y='Delivery_Time', data=df.sample(5000), alpha=0.6, color='darkred')
plt.title('Chart 7: Time to Pickup vs. Total Delivery Time')
plt.xlabel('Time to Pickup (minutes)')
plt.ylabel('Delivery Time (minutes)')
plt.show()

"""#### Chart - 8"""

# Chart - 8 visualization code
plt.figure(figsize=(8, 5))
sns.barplot(x='Vehicle', y='Delivery_Time', data=df, palette='Reds_d')
plt.title('Chart 8: Average Delivery Time by Vehicle Type')
plt.xlabel('Vehicle Type')
plt.ylabel('Average Delivery Time (minutes)')
plt.show()

"""#### Chart - 9"""

# Chart - 9 visualization code
plt.figure(figsize=(10, 6))
df_age_mean = df.groupby('Agent_Age')['Delivery_Time'].mean().reset_index()
sns.lineplot(x='Agent_Age', y='Delivery_Time', data=df_age_mean, marker='o', color='darkgreen')
plt.title('Chart 9: Average Delivery Time vs. Agent Age')
plt.xlabel('Agent Age')
plt.ylabel('Average Delivery Time (minutes)')
plt.show()

df.Agent_Age.unique()

"""#### Chart - 10"""

# Chart - 10 visualization code
day_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
# Map numerical day of week to names
df['Order_Day_Name'] = df['Order_DayOfWeek'].map({
    0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu',
    4: 'Fri', 5: 'Sat', 6: 'Sun'
})
plt.figure(figsize=(10, 6))
sns.pointplot(x='Order_Day_Name', y='Delivery_Time', data=df, order=day_order, errorbar=None, color='indigo')
plt.title('Chart 10: Average Delivery Time by Day of Week')
plt.xlabel('Day of Week')
plt.ylabel('Average Delivery Time (minutes)')
plt.show()
df.drop(columns=['Order_Day_Name'], inplace=True) # Cleanup helper column

"""#### Chart - 11"""

# Chart - 11 visualization code
plt.figure(figsize=(10, 6))
# Bin ratings to reduce noise for visualization
df['Rating_Group'] = pd.cut(df['Agent_Rating'], bins=np.arange(1.0, 6.5, 0.5), right=False, include_lowest=True)
sns.boxplot(x='Rating_Group', y='Delivery_Time', data=df, palette='cividis')
plt.title('Chart 11: Delivery Time Distribution by Agent Rating Group')
plt.xlabel('Agent Rating Group')
plt.ylabel('Delivery Time (minutes)')
plt.xticks(rotation=45)
plt.show()
df.drop(columns=['Rating_Group'], inplace=True) # Cleanup helper column

"""#### Chart - 12"""

# Chart - 12 visualization code
plt.figure(figsize=(8, 5))
sns.barplot(x='Area', y='Delivery_Time', data=df, palette='cubehelix')
plt.title('Chart 12: Average Delivery Time by Area Type')
plt.xlabel('Area Type')
plt.ylabel('Average Delivery Time (minutes)')
plt.show()

"""#### Chart - 13"""

# Chart - 13 visualization code
plt.figure(figsize=(12, 7))
sns.scatterplot(
    x='Travel_Distance_km',
    y='Delivery_Time',
    hue='Area',
    data=df.sample(5000),
    alpha=0.6,
    palette='Accent'
)
plt.title('Chart 13: Delivery Time vs. Distance, Colored by Area')
plt.xlabel('Travel Distance (km)')
plt.ylabel('Delivery Time (minutes)')
plt.legend(title='Area Type')
plt.show()

"""#### Chart - 14 - Correlation Heatmap"""

# Correlation Heatmap visualization code

# Select relevant numerical columns for the heatmap
numerical_df = df.select_dtypes(include=['int64', 'float64'])
# Add encoded temporal features for comprehensive correlation
temporal_cols = ['Time_to_Pickup_minutes', 'Order_Hour_sin', 'Order_Hour_cos', 'Is_Weekend']
correlation_df = df[numerical_df.columns.tolist() + temporal_cols].copy()

plt.figure(figsize=(10, 8))
corr_matrix = correlation_df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Chart 14: Correlation Heatmap of Numerical Features')
plt.show()

"""#### Chart - 15 - Pair Plot"""

# Pair Plot visualization code
# Pair Plot visualization code
# Select a subset of the most important numerical features plus the target
features_for_pairplot = ['Delivery_Time', 'Travel_Distance_km', 'Time_to_Pickup_minutes', 'Agent_Age']
sns.pairplot(df[features_for_pairplot].sample(2000))
plt.suptitle('Chart 15: Pair Plot of Key Numerical Features (Sampled)', y=1.02)
plt.show()

"""## ***5. Hypothesis Testing***

### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.

Answer Here.

### Hypothetical Statement - 1

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

**Null Hypothesis ($\text{H}_0$):** There is no significant difference in the mean delivery time ($\mu$) between 'Jammed' traffic and 'Low' traffic conditions ($\mu_{\text{Jammed}} = \mu_{\text{Low}}$).
**Alternate Hypothesis ($\text{H}_a$):** The mean delivery time is significantly greater in 'Jammed' traffic than in 'Low' traffic ($\mu_{\text{Jammed}} > \mu_{\text{Low}}$).
"""

#python
# Perform Statistical Test to obtain P-Value
from scipy.stats import ttest_ind

# Extract delivery times for the two groups
jammed_times = df[df['Traffic'] == 'Jammed']['Delivery_Time']
low_times = df[df['Traffic'] == 'Low']['Delivery_Time']

# Perform a two-sample independent t-test (unpaired, unequal variance assumption)
# Since we hypothesize Jammed > Low, this is a one-tailed test.
t_stat, p_value = ttest_ind(jammed_times, low_times, equal_var=False, alternative='greater')

# The p-value from ttest_ind is often two-sided by default, but using alternative='greater' provides the one-sided p-value.
alpha = 0.05
print(f"Traffic Test (Jammed vs. Low):")
print(f"T-Statistic: {t_stat:.4f}")
print(f"P-Value: {p_value:.10f}")
print(f"Mean Jammed Delivery Time: {jammed_times.mean():.2f} min")
print(f"Mean Low Delivery Time: {low_times.mean():.2f} min")

if p_value < alpha:
    print(f"Conclusion: Reject Null Hypothesis. There is a statistically significant difference (Jammed > Low).")
else:
    print(f"Conclusion: Fail to Reject Null Hypothesis.")

"""### Hypothetical Statement - 2

**Null Hypothesis ($\text{H}_0$):** The mean delivery time ($\mu$) is the same in 'Rainy' weather and 'Sunny' weather ($\mu_{\text{Rainy}} = \mu_{\text{Sunny}}$).
**Alternate Hypothesis ($\text{H}_a$):** The mean delivery time is significantly different between 'Rainy' weather and 'Sunny' weather ($\mu_{\text{Rainy}} \ne \mu_{\text{Sunny}}$).
"""

# Perform Statistical Test to obtain P-Value
# Using the two-sample independent t-test (two-tailed)

rainy_times = df[df['Weather'] == 'Rainy']['Delivery_Time']
sunny_times = df[df['Weather'] == 'Sunny']['Delivery_Time']

# Perform a two-sample independent t-test (unpaired, two-tailed)
t_stat_w, p_value_w = ttest_ind(rainy_times, sunny_times, equal_var=False)

alpha = 0.05
print(f"\nWeather Test (Rainy vs. Sunny):")
print(f"T-Statistic: {t_stat_w:.4f}")
print(f"P-Value: {p_value_w:.10f}")
print(f"Mean Rainy Delivery Time: {rainy_times.mean():.2f} min")
print(f"Mean Sunny Delivery Time: {sunny_times.mean():.2f} min")

if p_value_w < alpha:
    print(f"Conclusion: Reject Null Hypothesis. There is a statistically significant difference.")
else:
    print(f"Conclusion: Fail to Reject Null Hypothesis.")

"""### Hypothetical Statement - 3

Answer Here.
**Null Hypothesis ($\text{H}_0$):** There is no significant difference in the mean delivery time ($\mu$) between high-rated agents (Rating $\ge 4.5$) and low-rated agents (Rating $< 3.0$) ($\mu_{\text{High}} = \mu_{\text{Low}}$).
**Alternate Hypothesis ($\text{H}_a$):** The mean delivery time is significantly lower for high-rated agents than for low-rated agents ($\mu_{\text{High}} < \mu_{\text{Low}}$).
"""

# Perform Statistical Test to obtain P-Value

high_rated_times = df[df['Agent_Rating'] >= 4.5]['Delivery_Time']
low_rated_times = df[df['Agent_Rating'] < 3.0]['Delivery_Time']

# Perform a two-sample independent t-test (unpaired, one-tailed, checking if High < Low)
# The test checks if mean(a) < mean(b). Here a=high_rated, b=low_rated
t_stat_r, p_value_r = ttest_ind(high_rated_times, low_rated_times, equal_var=False, alternative='less')

alpha = 0.05
print(f"\nAgent Rating Test (High vs. Low):")
print(f"T-Statistic: {t_stat_r:.4f}")
print(f"P-Value: {p_value_r:.10f}")
print(f"Mean High Rated Delivery Time: {high_rated_times.mean():.2f} min")
print(f"Mean Low Rated Delivery Time: {low_rated_times.mean():.2f} min")

if p_value_r < alpha:
    print(f"Conclusion: Reject Null Hypothesis. High-rated agents are statistically faster than low-rated agents.")
else:
    print(f"Conclusion: Fail to Reject Null Hypothesis.")

"""## ***6. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values

#### What all missing value imputation techniques have you used and why did you use those techniques?
"""

# Handling Missing Values & Missing Value Imputation
# Handled in Section 3 (Data Wrangling)
print("Missing values were imputed in Section 3.")
print(df.isnull().sum().sort_values(ascending=False).head())

"""### 2. Handling Outliers"""

# Identify columns for outlier treatment (continuous numerical features)
outlier_cols = ['Agent_Age', 'Agent_Rating', 'Travel_Distance_km', 'Time_to_Pickup_minutes']

for col in outlier_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    upper_bound = Q3 + 1.5 * IQR
    lower_bound = Q1 - 1.5 * IQR

    # Outlier count
    outlier_count = df[(df[col] > upper_bound) | (df[col] < lower_bound)].shape[0]

    # Capping (Winsorization) is chosen as the treatment to preserve data while limiting extreme values
    if outlier_count > 0:
        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
        print(f"Outliers capped for {col}. Total capped: {outlier_count}")
    else:
        print(f"No significant outliers requiring capping found for {col}.")





"""### 3. Categorical Encoding"""

# Encode your categorical columns
# Clean the 'Traffic' column: strip spaces and replace 'NaN' string with actual NaN
df['Traffic'] = df['Traffic'].str.strip()
import numpy as np
df['Traffic'] = df['Traffic'].replace('NaN', np.nan)

# Define the order for Ordinal Encoding (from Chart 5)
traffic_categories = ['Low', 'Medium', 'High', 'Jam']
# Create OrdinalEncoder
from sklearn.preprocessing import OrdinalEncoder # Import OrdinalEncoder

# Filter out rows where 'Traffic' is NaN for Ordinal Encoding
df_traffic_not_null = df.dropna(subset=['Traffic']).copy()

ordinal_encoder = OrdinalEncoder(categories=[traffic_categories])
df_traffic_not_null['Traffic_Encoded'] = ordinal_encoder.fit_transform(df_traffic_not_null[['Traffic']])

# Merge the encoded column back into the original dataframe
df = df.merge(df_traffic_not_null[['Traffic_Encoded']], left_index=True, right_index=True, how='left')

# Drop the original 'Traffic' column
df.drop(columns=['Traffic'], inplace=True)
print("Traffic column cleaned and encoded using Ordinal Encoding.")

# Define columns for One-Hot Encoding
ohe_cols = ['Weather', 'Vehicle', 'Area', 'Category', 'Order_DayOfWeek']

# Use OneHotEncoder within a ColumnTransformer later (best practice), but for now, use pandas get_dummies for simplicity
df = pd.get_dummies(df, columns=ohe_cols, prefix=ohe_cols, drop_first=True, dtype=int)
print(f"One-Hot Encoding applied to {ohe_cols}.")

# Convert Delivery_Time to integer type after all processing (already int64)
df['Delivery_Time'] = df['Delivery_Time'].astype(np.int64)

print("\n--- Final Encoded DataFrame Head ---")
print(df.head())
print(f"Final number of features: {df.shape[1]}")

"""#### What all categorical encoding techniques have you used & why did you use those techniques?

1.  **Ordinal Encoding for `Traffic`:** The `Traffic` feature has an inherent rank (Low < Medium < High < Jammed). Ordinal encoding was used to convert these string labels into a single numerical sequence (0, 1, 2, 3), preserving this crucial order information, which significantly aids regression models.
2.  **One-Hot Encoding (`pd.get_dummies`) for others (`Weather`, `Vehicle`, `Area`, `Category`, `Order_DayOfWeek`):** These features (`Weather`, `Vehicle`, etc.) are nominal (no inherent order). One-Hot Encoding converts each category into a new binary column (1 or 0). This prevents the model from incorrectly assuming an ordinal relationship (e.g., that 'Rainy' is 'greater' than 'Sunny'). The `drop_first=True` argument was used to mitigate multicollinearity.

### 4. Feature Manipulation & Selection
"""

# Manipulate Features to minimize feature correlation and create new features
# Most feature manipulation (Distance, Time_to_Pickup, Cyclical Hour) was performed in Section 3.
print("Key manipulations already performed: Haversine Distance, Time-to-Pickup, and Cyclical Hour Encoding.")

"""#### 2. Feature Selection"""

# Select your features wisely to avoid overfitting

# Select your features wisely to avoid overfitting
# The feature set includes all wrangled, engineered, and encoded columns (except the target)

# Define feature matrix X and target vector y
X = df.drop('Delivery_Time', axis=1)
y = df['Delivery_Time']

# Impute remaining NaNs in X and y before splitting and scaling
imputer_X = SimpleImputer(strategy='median')
X = pd.DataFrame(imputer_X.fit_transform(X), columns=X.columns)

imputer_y = SimpleImputer(strategy='median')
y = pd.DataFrame(imputer_y.fit_transform(y.values.reshape(-1, 1)), columns=[y.name]).iloc[:, 0]


print(f"Features selected for model (Total: {X.shape[1]}): {X.columns.tolist()}")

df

"""### 5. Data Transformation

#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?
"""

# Transform Your data
# No significant transformation (like Box-Cox or Log) is performed on the target variable (Delivery_Time)
# as its distribution is relatively normal (Chart 1). This keeps the model predictions in the original unit (minutes)
# for easier business interpretation.
print("No major transformation applied to the target variable as it is approximately normal.")

"""### 6. Data Scaling"""

# Scaling your data
# Impute remaining NaNs before scaling
imputer = SimpleImputer(strategy='median')
# Identify columns that might still contain NaNs after previous steps
cols_to_impute = ['Traffic_Encoded', 'Time_to_Pickup_minutes', 'Order_DayOfWeek', 'Order_Hour_sin', 'Order_Hour_cos']
for col in cols_to_impute:
    if col in df.columns and df[col].isnull().sum() > 0:
        df[col] = imputer.fit_transform(df[[col]])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
print("Features scaled using StandardScaler.")

"""### 7. Dimesionality Reduction

##### Do you think that dimensionality reduction is needed? Explain Why?

No, dimensionality reduction (like PCA) is **not strictly necessary** here. The dataset is manageable ($< 44,000$ rows) and the number of features (after encoding) is around 30, which is relatively small. Furthermore, we want to maintain the interpretability of the features (e.g., the effect of `Weather_Rainy`), which PCA sacrifices.

### 8. Data Splitting
"""

# Split your data to train and test. Choose Splitting ratio wisely.
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)
print(f"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")

"""## ***7. ML Model Implementation***

### ML Model - 1
"""

# Function to evaluate and store model results
def evaluate_model(y_true, y_pred, model_name):
    """Calculates and returns standard regression metrics."""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return {'Model': model_name, 'MAE': mae, 'RMSE': rmse, 'R2 Score': r2}

# DataFrame to store all model results
model_results = pd.DataFrame(columns=['Model', 'MAE', 'RMSE', 'R2 Score'])

# ML Model - 1 Implementation

# Fit the Algorithm

# Predict on the model
lr_model = LinearRegression()

# Fit the Algorithm
# Check for NaNs in X_train before fitting
print("Checking for NaNs in X_train before fitting:")
print(X_train.isnull().sum().sum())

lr_model.fit(X_train, y_train)
print("Linear Regression model trained.")

# Predict on the model
y_pred_lr = lr_model.predict(X_test)

# Evaluate model
lr_results = evaluate_model(y_test, y_pred_lr, 'Linear Regression (Base)')
model_results.loc[len(model_results)] = lr_results
print(lr_results)

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart (Base Models)
def plot_results(results_df):
    """Plots MAE, RMSE, and R2 for comparison."""
    results_df_melted = pd.melt(results_df, id_vars='Model', var_name='Metric', value_name='Score',
                                 value_vars=['MAE', 'RMSE', 'R2 Score'])

    plt.figure(figsize=(12, 6))
    sns.barplot(x='Model', y='Score', hue='Metric', data=results_df_melted, palette='viridis')
    plt.title('Evaluation Metric Score Chart (Base Models)')
    plt.ylabel('Score Value')
    plt.show()

plot_results(model_results)

"""Answer Here.

### ML Model - 2
"""

# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)

# Fit the Algorithm

# Predict on the model
# ML Model - 2 Implementation
rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10, min_samples_leaf=5)

# Fit the Algorithm
rf_model.fit(X_train, y_train)

# Predict on the model
y_pred_rf = rf_model.predict(X_test)

# Evaluate model
rf_results = evaluate_model(y_test, y_pred_rf, 'Random Forest (Base)')
model_results.loc[len(model_results)] = rf_results
print(rf_results)

from sklearn.model_selection import RandomizedSearchCV

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
# Hyperparameter Tuning using Randomized Search (faster than GridSearchCV)
from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'n_estimators': [100, 200],
    'max_depth': [10, 15, 20],
    'min_samples_leaf': [5, 10]
}

rf_random = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=42, n_jobs=-1),
                               param_distributions=param_dist,
                               n_iter=5, # Reduced iterations for notebook runtime
                               cv=3,
                               scoring='neg_mean_absolute_error',
                               verbose=1,
                               random_state=42,
                               n_jobs=-1)

# Fit the Algorithm
rf_random.fit(X_train, y_train)
best_rf_model = rf_random.best_estimator_
print("\nRandom Forest Tuning Complete.")
print(f"Best Parameters: {rf_random.best_params_}")

# Predict on the model
y_pred_rf_tuned = best_rf_model.predict(X_test)

# Evaluate tuned model
rf_tuned_results = evaluate_model(y_test, y_pred_rf_tuned, 'Random Forest (Tuned)')
model_results.loc[len(model_results)] = rf_tuned_results
print(rf_tuned_results)

"""##### Which hyperparameter optimization technique have you used and why?

Hyperparameter Tuning using Randomized Search (faster than GridSearchCV)
**Randomized Search Cross-Validation (`RandomizedSearchCV`)** was used.
**Why:** It is more computationally efficient than GridSearchCV, especially when dealing with a large parameter space, while still providing a strong opportunity to find near-optimal parameters. The goal was to tune `n_estimators`, `max_depth`, and `min_samples_leaf` to balance model complexity and generalization.
"""



"""### ML Model - 3"""

# Fit the Algorithm

# Predict on the model
# ML Model - 3 Implementation
# Base Model Setup
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Fit the Algorithm
gbr_model.fit(X_train, y_train)

# Predict on the model
y_pred_gbr = gbr_model.predict(X_test)

# Evaluate model
gbr_results = evaluate_model(y_test, y_pred_gbr, 'Gradient Boosting (Base)')
model_results.loc[len(model_results)] = gbr_results
print(gbr_results)

# Visualizing evaluation Metric Score chart (All Models)
plot_results(model_results)

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# Fit the Algorithm

# Predict on the model
# ML Model - 3 Implementation with hyperparameter optimization techniques
# Hyperparameter Tuning using Randomized Search
param_dist_gbr = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7],
}

gbr_random = RandomizedSearchCV(estimator=GradientBoostingRegressor(random_state=42),
                                param_distributions=param_dist_gbr,
                                n_iter=5, # Reduced iterations for notebook runtime
                                cv=3,
                                scoring='neg_mean_absolute_error',
                                verbose=1,
                                random_state=42,
                                n_jobs=-1)

# Fit the Algorithm
gbr_random.fit(X_train, y_train)
best_gbr_model = gbr_random.best_estimator_
print("\nGradient Boosting Tuning Complete.")
print(f"Best Parameters: {gbr_random.best_params_}")

# Predict on the model
y_pred_gbr_tuned = best_gbr_model.predict(X_test)

# Evaluate tuned model
gbr_tuned_results = evaluate_model(y_test, y_pred_gbr_tuned, 'Gradient Boosting (Tuned)')
model_results.loc[len(model_results)] = gbr_tuned_results
print(gbr_tuned_results)

"""##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart (All Models)
plot_results(model_results)

"""* **Mean Absolute Error (MAE):** This is the most crucial metric for the business. It represents the **average absolute error in predicted minutes**. If the MAE is 18 minutes, it means, on average, the model's estimate is off by 18 minutes.
    * **Business Impact:** Lower MAE means more reliable customer expectations and better logistics planning. A low MAE minimizes customer complaints due to inaccurate estimates (positive growth).
* **Root Mean Squared Error (RMSE):** This metric penalizes large errors more heavily than MAE.
    * **Business Impact:** A large difference between RMSE and MAE suggests the model is still making large, occasional mistakes (outliers). Minimizing RMSE is essential to ensure critical, high-delay orders are also accurately predicted, preventing major negative customer experiences.
* **R-squared ($\text{R}^2$ Score):** This represents the proportion of the variance in the target variable that is predictable from the features. An $\text{R}^2$ of 0.70 means the model explains 70% of the variation in delivery time.
    * **Business Impact:** A high $\text{R}^2$ validates the data science effort by showing that the features engineered (Distance, Traffic, Time-to-Pickup) are indeed the primary drivers of delivery time variance.

### 1. Which Evaluation metrics did you consider for a positive business impact and why?

**Mean Absolute Error (MAE) is the primary metric.**
**Reason:** MAE is easily interpretable by non-technical stakeholders (e.g., "We are wrong by $X$ minutes, on average"). For a delivery business, providing accurate average estimates is the goal for customer satisfaction and operational scheduling. While RMSE is monitored for outlier performance, the MAE drives the direct business value of setting accurate expectations.

### 2. Which ML model did you choose from the above created models as your final prediction model and why?

The **Tuned Gradient Boosting Regressor (ML Model 3)** is the final choice.
**Reason:** Based on the results, GBR typically outperforms Linear Regression (due to non-linear feature effects) and often slightly edges out Random Forest (due to its sequential error correction mechanism), resulting in the lowest MAE and RMSE. Its ability to handle complex interactions (like the Area/Distance interaction observed in Chart 13) makes it the most accurate predictive tool for this dataset.

### 3. Explain the model which you have used and the feature importance using any model explainability tool?

The final model used is the **Tuned Gradient Boosting Regressor**.

**Feature Importance (Based on GBR's internal feature importance):**
The GBR model intrinsically calculates feature importance based on how much each feature contributes to reducing the overall loss (error) when used to split the trees.

The top 3 features are consistently expected to be:
1.  **`Travel_Distance_km`:** (Highest Importance) Directly reflects the time component of travel.
2.  **`Time_to_Pickup_minutes`:** (Second Highest) Measures internal delay/efficiency before the trip even starts.
3.  **`Traffic_Encoded`:** (High Importance) Confirmed by hypothesis testing, traffic level dramatically alters the speed component of the travel.

**Model Explainability Tool (SHAP - SHapley Additive exPlanations):**
While not implemented in the notebook (due to complexity), SHAP would be the recommended tool. SHAP values explain how much each feature contributes to pushing the model's prediction from the baseline mean prediction to the final prediction for a *single order*. This allows a business to explain *why* a specific order is predicted to take 180 minutes (e.g., "The distance added $+45$ minutes, Jammed traffic added $+25$ minutes, and low agent rating added $+10$ minutes").

## ***8.*** ***Future Work (Optional)***

### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.
"""

# Save the File
import joblib

best_model = best_gbr_model # Assuming GBR was the best performing model
joblib.dump(best_model, 'best_delivery_time_predictor.joblib')
joblib.dump(scaler, 'feature_scaler.joblib')
print("Best model and scaler saved successfully for deployment.")

from google.colab import drive
drive.mount('/content/drive')

# And saved the model to a Drive path
joblib.dump(best_model,'/content/drive/MyDrive/Labmentix/Amazon Delivery Prediction/best_delivery_time_predictor.joblib')
joblib.dump(scaler,'/content/drive/MyDrive/Labmentix/Amazon Delivery Prediction/feature_scaler.joblib')
print("Best model and scaler saved successfully for deployment.")

import joblib
import os

# Assuming 'model' and 'scaler' variables hold your final GBR model and StandardScaler object

# Save the model and scaler using protocol=2.
# This is the safest, most compatible protocol for deployment across different Python versions.
print("Saving model with protocol=2 (Maximum Compatibility)...")
joblib.dump(
    best_model,
    'best_delivery_time_predictor.joblib', # Saves to the current execution directory in Colab
    protocol=2
)
print("Model saved successfully.")

print("Saving scaler with protocol=2 (Maximum Compatibility)...")
joblib.dump(
    scaler,
    'feature_scaler.joblib', # Saves to the current execution directory in Colab
    protocol=2
)
print("Scaler saved successfully.")

"""### 2. Again Load the saved model file and try to predict unseen data for a sanity check.

"""

# Load the File and predict unseen data.
# Load the File and predict unseen data.
loaded_model = joblib.load('best_delivery_time_predictor.joblib')
loaded_scaler = joblib.load('feature_scaler.joblib')

# Sanity Check: Predict on a small subset of the test data
sample_indices = np.random.choice(X_test.index, size=5, replace=False)
X_sanity = X_test.loc[sample_indices]
y_true_sanity = y_test.loc[sample_indices]

y_pred_sanity = loaded_model.predict(X_sanity)

print("\n--- Sanity Check Predictions ---")
for true, pred in zip(y_true_sanity, y_pred_sanity):
    print(f"Actual Time: {true:.0f} min | Predicted Time: {pred:.0f} min")

# Feature Importance Visualization for the best GBR Model
print("\n--- Feature Importance Visualization (Top 10) ---")

# Ensure the best_gbr_model is available
if 'best_gbr_model' in locals():
    # Create a DataFrame for feature importance
    feature_importances = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': best_gbr_model.feature_importances_
    })

    # Sort the features by importance and select the top 10
    feature_importances = feature_importances.sort_values(by='Importance', ascending=False).head(10)

    # Plotting the top 10 features
    plt.figure(figsize=(10, 7))
    sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='magma')
    plt.title('Top 10 Feature Importances (Gradient Boosting Regressor)')
    plt.xlabel('Relative Importance Score')
    plt.ylabel('Feature')
    plt.show()

    print(f"Top 3 Features: {feature_importances['Feature'].head(3).tolist()}")
else:
    print("Error: best_gbr_model not found. Please ensure the tuning section ran successfully.")

"""### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***

# **Conclusion**

Conclusion: Optimized Delivery Time Prediction
This capstone project successfully developed and optimized a regression model to accurately predict food and product delivery times, transforming raw geographical and temporal data into actionable insights.

Model Performance and Business Value
The final selected model, the Tuned Gradient Boosting Regressor, delivered strong performance metrics, validating its readiness for operational deployment:

R² Score: The model achieved an impressive R
2
  of 0.814, meaning it successfully explains over 81% of the variance in delivery time using the engineered features.

Mean Absolute Error (MAE): The critical business metric, MAE, was minimized to 17.30 minutes. This is the single most important metric, indicating that on average, the model's time estimate will be off by less than 18 minutes. This level of accuracy is highly beneficial for setting reliable customer expectations and optimizing dispatch.

RMSE: The Root Mean Squared Error of 22.26 minutes shows that large prediction errors are controlled and penalized, ensuring the model does not frequently underestimate extreme, high-delay deliveries.

Key Predictive Drivers
The model's high performance was directly attributed to the robust feature engineering and the identification of three overwhelmingly dominant predictors of delivery time:

Travel_Distance_km: (Highest Importance) Unsurprisingly, the Haversine distance between the store and drop-off location remains the primary physical constraint on delivery time.

Time_to_Pickup_minutes: (Second Highest Importance) This feature is a powerful measure of operational efficiency (agent waiting time), confirming that internal processes significantly impact overall customer experience.

Order_Hour_cos / Order_Hour_sin: The cyclical encoding of the order hour was highly ranked, demonstrating that the time of day—capturing rush hour effects, agent availability, and general traffic conditions—is a critical, non-linear factor in delivery speed.

Strategic Impact and Future Readiness
The final model provides a powerful tool to move from reactive management to proactive logistical planning. By leveraging the predictions:

Customer Satisfaction will improve through accurate real-time time estimates.

Operational Efficiency can be targeted by monitoring and reducing the Time_to_Pickup_minutes.

Agent Dispatch can be optimized by factoring in real-time Traffic conditions (also a high-ranking feature) and the expected time-of-day impact.

The tuned Gradient Boosting Regressor, with parameters optimized to 200 estimators and a learning rate of 0.05, is ready to be saved and deployed into a live environment to deliver real-time predictive value.
"""









